---
layout: post
title:  "Case Study 1: Row-wise equality checking"
date:   2021-11-11 07:00:00 +0200
categories: python timing speed-up optimization
---


## Introduction

It shouldn't come as a surprise that I like to optimize (Python) code. So when
a colleague, more experience in **R** than Python/Numpy, mentioned in a presentation
that a certain calculation was rather slow, I was intrigued whether I might be
able to help speed it up. In this blog post, I'll take you along in the process
I went through to optimize the code.

After confirming the code was indeed written in Python and asking for access,
I took a look at the function. Below is a modified version showcasing only
what was relevant to the optimization.

```python
def calculate(data, no_tests=100):
    n, d = data.shape  # typically n = 1_000, 10_000 or 100_000, d = 9

    for i in range(no_tests):
        for j in range(d):
            # prepare indices
            a_ind = np.concatenate([np.arange(0,j), np.arange(j+1,d)])
            # prepare auxiliary informationn of every individual to find peers
            aux = np.apply_along_axis(np.array2string,1, data[:,a_ind])
            # save data of peers
            peers_data = data[aux==aux[i],j]
    ...
```


## Step 1: What does it do?

At first glance, it's a double for-loop which creates some indices, uses them to
transforms some of the data into strings as auxiliary data, and performs a
selection based on equality in that auxiliary data. The first thing that stands
out to me here is the `np.array2string` function call, as such a transformation
is rather expensive, so I wonder if that is really needed. If not, that would
probably be the place to start.

But let's not get ahead of ourselves. Let's consider what this function does
step-by-step. Let's set up a toy example with some sample data, pick some `i,j`
in the middle of the loops, flatten the function and progress through a single
run of the inner-most loop:

```python
import numpy as np                                    # intermediate values:
n, d = 3, 4
i, j = 1, 2
data = np.arange(n*d).reshape(n, d) / 100             # [[0.  , 0.01, 0.02, 0.03],
                                                      #  [0.04, 0.05, 0.06, 0.07],
                                                      #  [0.08, 0.09, 0.1 , 0.11]]

range_1, range_2 = np.arange(0, j), np.arange(j+1,d)  # [0, 1], [3]
a_ind = np.concatenate([range_1, range_2])            # [0, 1, 3]

tmp = data[:,a_ind]                                   # [[0.  , 0.01, 0.03],
                                                      #  [0.04, 0.05, 0.07],
                                                      #  [0.08, 0.09, 0.11]]

aux = np.apply_along_axis(np.array2string, 1, tmp)    # ['[0.   0.01 0.03]',
                                                      #  '[0.04 0.05 0.07]',
                                                      #  '[0.08 0.09 0.11]']
comparisons = aux==aux[i]                             # [False, True, False]

peers_data = data[comparisons, j]                     # [0.1]
```

Looking at the flattened code and intermediate values from this sample run, we
can see the process performed by the function much more clearly: In short, we
exclude column `j` from the data and check whether any row is equal to the
selected row `i`. The original data in column `j` for each of those rows is then
used for further calculations.


## Step 2: What to Optimize

Now we know what the function does, we can try to optimize it. There are two candidate statements to optimize: creating the `a_ind` indices and determining which rows are equal using `aux`. Let's test them both:

```python
j, d = 3, 9
%timeit np.concatenate([range(0,j), range(j+1,d)])
```
> 7.18 µs ± 310 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

```python
large_data = np.random.randn(1_000, 9)  # test speed using larger dataset
%timeit aux = np.apply_along_axis(np.array2string, 1, large_data); aux == aux[0]
```
> 130 ms ± 3.14 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

The equality check using `np.array2string` takes a factor of 20,000 more time than creating the indices. So as expected, that's going to be the place to start.


## Step 3: Optimizing within inner loop

### Replacing _np.array2string_

We can reuse the `np.apply_along_axis` function to check for element-wise equality between rows with `np.allclose`. This returns a single `True` or `False` when comparing two rows, just like with the strings previously. This results in a 4x speed-up:

```python
comparisons = np.apply_along_axis(np.allclose, 1, tmp, tmp[i])
# [False, True, False]
%timeit np.apply_along_axis(np.allclose, 1, large_data, large_data[0])
```
> 32.8 ms ± 1.76 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)<br>
> -> 3.9x speed-up


### _np.allclose_ vs _np.equal_

If we don't need the tolerance-checking by `np.allclose`, we can even try just using `np.equal` instead. However, this returns the _element-wise_ results, not a single `True` or `False` for each row:

```python
comparisons = np.apply_along_axis(np.equal, 1, tmp, tmp[i])
# [[False, False, False],
#  [ True,  True,  True],
#  [False, False, False]]
```

To end up with the correct result, each row has to be summarized into `True` if the whole row is `True`, otherwise the row should be summarized to `False`. For this, we can use the `np.all` function, which works like an array-wide boolean **AND** operation.

```python
np.all([
    [ True,  True,  True],
    [ True,  True, False],
    [False, False, False],
], axis=1)
# [ True, False, False]

comparisons = np.all(np.apply_along_axis(np.equal, 1, tmp, tmp[i]), axis=1)
# [False, True, False]
%timeit np.all(np.apply_along_axis(np.equal, 1, large_data, large_data[0]), axis=1)
```
> 2.03 ms ± 59.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)<br>
> -> 64x speed-up

Perhaps surprisingly, this is another 16x faster! Even when adding the extra call of `np.all`, it turns out that `np.equal` is _much_ faster than `np.allclose`, most likely because of the tolerance checks.


### Removing _np.apply_along_axis_

Because of numpy's broadcasting rules, we don't actually need the `np.apply_along_axis` function to perform the element-wise comparison per row of the data. We can simply compare the entire array with the relevant row.

```python
tmp == tmp[i]  # same as np.equal
# [[False, False, False],
#  [ True,  True,  True],
#  [False, False, False]]
np.all(tmp == tmp[i], axis=1)
# [False, True, False]

%timeit np.all(large_data == large_data[0], axis=1)
```
> 36.8 µs ± 2.06 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)<br>
> -> 3,500x speed-up

That's a 3,500x speedup in total! Turns out that numpy by itself is pretty fast at performing these kinds of comparisons. With that, we've both simplified the code and made it a lot faster.


## Step 4: Factoring code out of loops

### Inner loop

So far, we've optimized specifically the most expensive statements. But if you recall from the start of our optimization journey, this code is executed within a loop:

```python
# optimized code
for j in range(d):
    a_ind = np.concatenate([range(0,j), range(j+1,d)])
    comparisons = np.all(data[:,a_ind] == data[i,a_ind], axis=1)

```

To calculate `comparisons`, only the columns of the `a_ind` indices are used. As `a_ind` only excludes a single column per iteration, each column is compared `d-1` times. We can check this by printing `a_ind` for each iteration:

```python
for j in range(d):
    a_ind = np.concatenate([range(0,j), range(j+1,d)])
    print(a_ind)
# [1 2 3]
# [0 2 3]
# [0 1 3]
# [0 1 2]

%%timeit
for i in range(10):
    for j in range(9):
        a_ind = np.concatenate([np.arange(0,j), np.arange(j+1,9)])
        comparisons = np.all(large_data[:,a_ind] == large_data[i,a_ind], axis=1)
```
> 3.51 ms ± 109 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

The element-wise comparisons in `data[:,a_ind] == data[i,a_ind]` won't change between iterations, so we're actually making the same comparisons `d-1` times! That makes this a good candidate for reintroducing an auxiliary variable, but this time _outside_ of the inner loop:

```python
%%timeit
aux = large_data == large_data[i]
for j in range(9):
    a_ind = np.concatenate([np.arange(0,j), np.arange(j+1,9)])
    comparisons = np.all(aux[a_ind], axis=1)
```
> 1.24 ms ± 65.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)<br>
> -> 2.7x speed-up

### Outer loop

We can even extract the `a_ind` creation out of all loops. After all, the set of column-exclusion indices can be reused in each iteration of the outer loop. This last step might not be as important, but as the size of `large_data` increases to potentially millions, those microseconds per iteration can still add up. Besides, I think it makes the code cleaner: it clearly separates what _needs_ to be done within the loop from what does not.

```python
%%timeit
a_indices = np.array([
    np.concatenate([np.arange(0,j), np.arange(j+1,d)])
    for j in range(d)
])
for i in range(n):
    aux = large_data == large_data[i]
    for j, a_ind in enumerate(a_indices):
        comparisons = np.all(aux[a_ind], axis=1)
```
> 860 µs ± 41.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)<br>
> -> 4x speed-up


## Step 5: Parallelize and benchmark

As a final step, we can see that the code within the outer loop is actually trivially parallelizable, as it only depends on `i` as differing input. It makes the code a bit more cluttered by adding some imports and a helper function, but can cut down execution time from minutes to seconds for large datasets.

<details markdown=block>
<summary markdown=span>Benchmarking code</summary>

```python
from functools import partial
from multiprocessing import Pool, cpu_count
from time import time
import numpy as np
import matplotlib.pyplot as plt

def calc_orig(data):
    """Original implementation"""
    n, d = data.shape
    results = np.empty((n,d))
    for i in range(n):
        for j in range(d):
            a_ind = np.concatenate([np.arange(0,j), np.arange(j+1,d)])
            aux = np.apply_along_axis(np.array2string,1, data[:,a_ind])
            peers_data = data[aux==aux[i],j]
            results[i,j] = np.sum(peers_data)
    return results

def calc_inner_loop_optimized(data):
    """Optimized code within inner loop"""
    n, d = data.shape
    results = np.empty((n,d))
    for i in range(n):
        for j in range(d):
            a_ind = np.concatenate([np.arange(0,j), np.arange(j+1,d)])
            comparisons = np.all(data[:,a_ind] == data[i,a_ind], axis=1)
            results[i,j] = np.sum(comparisons)
    return results

def calc_optimized(data):
    """Optimized code, including extractions from loop"""
    n, d = data.shape
    a_indices = np.array([
        np.concatenate([np.arange(0,j), np.arange(j+1,d)])
        for j in range(d)
    ])
    results = np.empty((n,d))
    for i in range(n):
        aux = data == data[i]
        for j, a_ind in enumerate(a_indices):
            comparisons = np.all(aux[a_ind], axis=1)
            results[i,j] = np.sum(comparisons)
    return results

def _parallel_helper(i, data, a_indices):
    """Helper function to perform single run for parallelization"""
    n, d = data.shape
    aux = data == data[i]
    results = np.empty(d)
    for j, a_ind in enumerate(a_indices):
        comparisons = np.all(aux[a_ind], axis=1)
        results[j] = np.sum(comparisons)

def calc_parallel(data, n_procs=cpu_count()):
    """Parallel implementation"""
    n, d = data.shape
    a_indices = np.array([
        np.concatenate([np.arange(0,j), np.arange(j+1,d)])
        for j in range(d)
    ])
    parallel_func = partial(_parallel_helper, data=data, a_indices=a_indices)
    with Pool(n_procs) as p:
        result = p.map(parallel_func, range(n))
    return np.array(result)

if __name__ == '__main__':
    magnitudes = range(1, 6)
    numbers = [1, 2, 5]
    N = [n * 10**m for m in magnitudes for n in numbers]
    functions = {
        'original': calc_orig,
        'inner loop optimized': calc_inner_loop_optimized,
        'fully optimized': calc_optimized,
        'parallel (4)': partial(calc_parallel, n_procs=4),
        'parallel (8)': partial(calc_parallel, n_procs=8),
        'parallel (12)': partial(calc_parallel, n_procs=12),
    }
    durations = {name: np.full(len(N), np.nan) for name in functions.keys()}
    skip = {name: False for name in functions.keys()}

    for idx, n in enumerate(N):
        print(n)
        data = np.random.randn(n, 9)
        for name, func in functions.items():
            if skip[name]:
                continue
            start = time()
            func(data)
            end = time()
            durations[name][idx] = end-start
            print(f'    {name}: {durations[name][idx]}')
            if durations[name][idx] > 100:
                skip[name] = True

    fig, ax = plt.subplots()
    ax.set_title('Speed comparison of implementations')
    ax.set_xscale('log')
    ax.set_xlabel('$n$')
    ax.set_yscale('log')
    ax.set_ylabel('time (s)')
    for name, duration in durations.items():
        ax.plot(N, duration, marker='+', ms=12, label=name)
    ax.legend(loc='lower right')
    ax.grid(which='major', color='black', alpha=.5, linestyle='-')
    ax.grid(which='minor', color='black', alpha=.25, linestyle=':')
    fig.savefig('case-study-1-benchmark.png')  # or .pdf or .svg
```

</details>

![](/img/case-study-1-benchmark.svg)

## Conclusion

And so we've gone from an **R**-inspired string-based implementation past a clean and minimal pure-Python one all the way to a fully optimized and parallelized implementation that can scale over many cores. I've had a lot of fun figuring out all these optimizations, and I hope you enjoyed coming along on this optimization journey with me.

Finally, I would like to thank my colleague for being open to having me tinker on their code.