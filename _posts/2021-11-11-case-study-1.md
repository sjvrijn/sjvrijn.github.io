---
layout: post
title:  "Case Study 1: Row-wise equality checking"
date:   2021-11-11 07:00:00 +0200
categories: python timing speedup optimization
---


## Introduction

It shouldn't come as a surprise that I like to optimize (Python) code. So when
a colleague (who is more experienced in **R** than Python/Numpy) mentioned in a
presentation that a certain calculation was rather slow, I was intrigued. Would
I be able to help speed it up? In this blog post, I'll take you along in
the process I went through to optimize the code.

After confirming the code was indeed written in Python and asking for access,
I took a look at the function. Below is a modified version showcasing what was
relevant to the optimization.

```python
def calculate(data, no_tests=100):
    n, d = data.shape  # typically n = 1_000, 10_000 or 100_000, d = 9

    for i in range(no_tests):
        for j in range(d):
            # prepare indices
            a_ind = np.concatenate([np.arange(0,j), np.arange(j+1,d)])
            # prepare auxiliary informationn of every individual to find peers
            aux = np.apply_along_axis(np.array2string,1, data[:,a_ind])
            # save data of peers
            peers_data = data[aux==aux[i],j]
    ...
```


## Step 1: What does it do?

At first glance, it's a double for-loop which creates some indices, uses them to
transforms some of the data into strings as auxiliary data, and performs a
selection based on equality in that auxiliary data. The first thing that stands
out to me here is the `np.array2string` function call, as such a transformation
is rather expensive, so I wonder if that is really needed. If not, that would
probably be the place to start.

But let's not get ahead of ourselves. Let's consider what this function does
step-by-step. Let's set up a toy example with some sample data, pick some `i,j`
in the middle of the loops, flatten the function and progress through a single
run of the inner-most loop:

```python
import numpy as np                                    # intermediate values:
n, d = 3, 4
i, j = 1, 2
data = np.arange(n*d).reshape(n, d) / 100             # [[0.  , 0.01, 0.02, 0.03],
                                                      #  [0.04, 0.05, 0.06, 0.07],
                                                      #  [0.08, 0.09, 0.1 , 0.11]]

range_1, range_2 = np.arange(0, j), np.arange(j+1,d)  # [0, 1], [3]
a_ind = np.concatenate([range_1, range_2])            # [0, 1, 3]

tmp = data[:,a_ind]                                   # [[0.  , 0.01, 0.03],
                                                      #  [0.04, 0.05, 0.07],
                                                      #  [0.08, 0.09, 0.11]]

aux = np.apply_along_axis(np.array2string, 1, tmp)    # ['[0.   0.01 0.03]',
                                                      #  '[0.04 0.05 0.07]',
                                                      #  '[0.08 0.09 0.11]']
comparisons = aux==aux[i]                             # [False, True, False]

selected = data[comparisons, j]                       # [0.1]
```

Looking at the flattened code and intermediate values from this sample run, we
can see the process performed by the function much more clearly: In short, we
exclude column `j` from the data and check whether any rows are equal to the
selected row `i`. The original data in column `j` for each of those rows is then
used for further calculations.


## Step 2: What to Optimize

Now we know what the function does, we can try to optimize it. There are two
candidate statements to optimize: creating the `a_ind` indices and determining
which rows are equal using `aux`. Let's test them both:

```python
j, d = 3, 9
%timeit np.concatenate([range(0,j), range(j+1,d)])
```
> 7.18 µs ± 310 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

```python
large_data = np.random.randn(1_000, 9)  # test speed using larger dataset
%timeit aux = np.apply_along_axis(np.array2string, 1, large_data); aux == aux[0]
```
> 130 ms ± 3.14 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

The equality check using `np.array2string` takes a factor of 20,000 more
time than creating the indices. So as expected, that's going to be the place
to start. When comparing the speed, we'll keep using the 1,000 x 9 `large_data`
array we created here. This is large enough to give a good idea of the speed,
but small enough to make testing fast.


## Step 3: Optimizing within loops

The `np.apply_along_axis(np.array2string, 1, large_data)` statement is not just
the slowest statement we found, but it's extra worthwhile to optimize since it's
within the double nested loop. Any small improvement to a single run in that
statement will add up quickly when run so many times in the loops. This means
that, for now, we can just focus on optimizing that single statement as-is,
without looking at the loops too.


### Replacing _np.array2string_

First step is replacing row-equality check to no longer use strings.
Instead of using `np.array2string`, we can check for element-wise equality
between rows with `np.allclose`. This returns a single `True` or `False` when
comparing two rows, just like with the strings previously. This results in a
4x speedup:

```python
comparisons = np.apply_along_axis(np.allclose, 1, tmp, tmp[i])
# [False, True, False]
%timeit np.apply_along_axis(np.allclose, 1, large_data, large_data[0])
```
> 32.8 ms ± 1.76 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)<br>
> -> 3.9x speedup


### _np.allclose_ vs _np.equal_

If we don't need the tolerance-checking by `np.allclose`, we can even try just
using `np.equal` instead. However, this returns the _element-wise_ results, not
a single `True` or `False` for each row:

```python
comparisons = np.apply_along_axis(np.equal, 1, tmp, tmp[i])
# [[False, False, False],
#  [ True,  True,  True],
#  [False, False, False]]
```

To end up with the correct result, each row has to be summarized into `True` if
the whole row is `True`, otherwise the row should be summarized to `False`. For
this, we can use the `np.all` function, which works like an array-wide boolean
AND operation.

```python
# for example:
np.all([
    [ True,  True,  True],
    [ True,  True, False],
    [False, False, False],
], axis=1)
# [ True, False, False]

comparisons = np.all(np.apply_along_axis(np.equal, 1, tmp, tmp[i]), axis=1)
# [False, True, False]
%timeit np.all(np.apply_along_axis(np.equal, 1, large_data, large_data[0]), axis=1)
```
> 2.03 ms ± 59.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)<br>
> -> 64x speedup

Perhaps surprisingly, this is another 16x faster! Even when adding the extra
`np.all` call, it turns out that `np.equal` is _much_ faster than `np.allclose`.


### Removing _np.apply_along_axis_

Because of numpy's broadcasting rules, we don't actually need the
`np.apply_along_axis` function to perform the element-wise comparison per row
of data. We can simply compare the entire array with the relevant row. Note that
this would work for `np.allclose` too.

```python
tmp == tmp[i]  # same as np.equal
# [[False, False, False],
#  [ True,  True,  True],
#  [False, False, False]]
np.all(tmp == tmp[i], axis=1)
# [False, True, False]

%timeit np.all(large_data == large_data[0], axis=1)
```
> 36.8 µs ± 2.06 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)<br>
> -> 3,500x speedup

That's a 3,500x speedup in total! Turns out that numpy by itself is pretty fast
at performing these kinds of comparisons. With that, we've both simplified the
code and made it a lot faster.

You may also note that this version only takes ~5x as long as the index creation
code we also timed earlier. It's less worth it to spend time on this however, as
it does not scale with the amount of data rows, and we can speed it up using a
different technique in the next step anyway.


## Step 4: Factoring code out of loops

We've now made the code within the loops as fast as possible. The next step is
to see if there is any work we're unnecessarily doing again in any of the loops
and well... making sure we don't.


### Inner loop

Remember that the inner loop was iterating over the 9 columns of our sample
data. Each time we create the necessary indices and compare the data with the
current row.

```python
for i in range(no_tests):  # Outer loop
    for j in range(d):     # Inner loop
        a_ind = np.concatenate([range(0,j), range(j+1,d)])
        comparisons = np.all(data[:,a_ind] == data[i,a_ind], axis=1)
```

The indices depend on the column `j`, so within this loop we're not doing any
extra work there. But to calculate `comparisons`, only the columns of the
`a_ind` indices are used. As `a_ind` only excludes a single column per
iteration, each column is compared `d-1` times. We can check this by printing
`a_ind` for each iteration:

```python
for j in range(d):
    a_ind = np.concatenate([range(0,j), range(j+1,d)])
    print(a_ind)
# [1 2 3]
# [0 2 3]
# [0 1 3]
# [0 1 2]

%%timeit
for i in range(10):
    for j in range(9):
        a_ind = np.concatenate([np.arange(0,j), np.arange(j+1,9)])
        comparisons = np.all(large_data[:,a_ind] == large_data[i,a_ind], axis=1)
```
> 3.51 ms ± 109 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

The element-wise comparisons in `data[:,a_ind] == data[i,a_ind]` won't change
between iterations, so we're actually making the same comparisons `d-1 = 8`
times! That makes this a good candidate for reintroducing an auxiliary variable,
but this time _outside_ of the inner loop:

```python
%%timeit
for i in range(10):
    aux = large_data == large_data[i]
    for j in range(9):
        a_ind = np.concatenate([np.arange(0,j), np.arange(j+1,9)])
        comparisons = np.all(aux[a_ind], axis=1)
```
> 1.24 ms ± 65.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)<br>
> -> 2.7x speedup

We're saving 7/8th of the comparisons compared to previously, limiting the
amount of work within the inner loop. But out of the potential 8x speedup, we
'only' get 2.7x.

### Outer loop

We can even extract the `a_ind` creation out of all loops. After all, the set of column-exclusion indices can be reused in each iteration of the outer loop. This last step might not be as important, but as the size of `large_data` increases to potentially millions, those microseconds per iteration can still add up. Besides, I think it makes the code cleaner: it clearly separates what _needs_ to be done within the loop from what does not.

```python
%%timeit
a_indices = np.array([
    np.concatenate([np.arange(0,j), np.arange(j+1,d)])
    for j in range(d)
])
for i in range(10):
    aux = large_data == large_data[i]
    for j, a_ind in enumerate(a_indices):
        comparisons = np.all(aux[a_ind], axis=1)
```
> 910 µs ± 24.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)<br>
> -> 3.8x speedup


## Step 5: Parallelize and benchmark

As a final step, we can see that the code within the outer loop is actually trivially parallelizable, as it only depends on `i` as differing input. It makes the code a bit more cluttered by adding some imports and a helper function, but can cut down execution time from minutes to seconds for large datasets.

<details markdown=block>
<summary markdown=span>Benchmarking code</summary>

```python
from functools import partial
from multiprocessing import Pool, cpu_count
from time import time
import numpy as np
import matplotlib.pyplot as plt

def calc_orig(data):
    """Original implementation"""
    n, d = data.shape
    results = np.empty((n,d))
    for i in range(n):
        for j in range(d):
            a_ind = np.concatenate([np.arange(0,j), np.arange(j+1,d)])
            aux = np.apply_along_axis(np.array2string,1, data[:,a_ind])
            peers_data = data[aux==aux[i],j]
            results[i,j] = np.sum(peers_data)
    return results

def calc_inner_loop_optimized(data):
    """Optimized code within inner loop"""
    n, d = data.shape
    results = np.empty((n,d))
    for i in range(n):
        for j in range(d):
            a_ind = np.concatenate([np.arange(0,j), np.arange(j+1,d)])
            comparisons = np.all(data[:,a_ind] == data[i,a_ind], axis=1)
            results[i,j] = np.sum(comparisons)
    return results

def calc_optimized(data):
    """Optimized code, including extractions from loop"""
    n, d = data.shape
    a_indices = np.array([
        np.concatenate([np.arange(0,j), np.arange(j+1,d)])
        for j in range(d)
    ])
    results = np.empty((n,d))
    for i in range(n):
        aux = data == data[i]
        for j, a_ind in enumerate(a_indices):
            comparisons = np.all(aux[a_ind], axis=1)
            results[i,j] = np.sum(comparisons)
    return results

def _parallel_helper(i, data, a_indices):
    """Helper function to perform single run for parallelization"""
    n, d = data.shape
    aux = data == data[i]
    results = np.empty(d)
    for j, a_ind in enumerate(a_indices):
        comparisons = np.all(aux[a_ind], axis=1)
        results[j] = np.sum(comparisons)

def calc_parallel(data, n_procs=cpu_count()):
    """Parallel implementation"""
    n, d = data.shape
    a_indices = np.array([
        np.concatenate([np.arange(0,j), np.arange(j+1,d)])
        for j in range(d)
    ])
    parallel_func = partial(_parallel_helper, data=data, a_indices=a_indices)
    with Pool(n_procs) as p:
        result = p.map(parallel_func, range(n))
    return np.array(result)

if __name__ == '__main__':
    magnitudes = range(1, 6)
    numbers = [1, 2, 5]
    N = [n * 10**m for m in magnitudes for n in numbers]
    functions = {
        'original': calc_orig,
        'inner loop optimized': calc_inner_loop_optimized,
        'fully optimized': calc_optimized,
        'parallel (4)': partial(calc_parallel, n_procs=4),
        'parallel (8)': partial(calc_parallel, n_procs=8),
        'parallel (12)': partial(calc_parallel, n_procs=12),
    }
    durations = {name: np.full(len(N), np.nan) for name in functions.keys()}
    skip = {name: False for name in functions.keys()}

    for idx, n in enumerate(N):
        print(n)
        data = np.random.randn(n, 9)
        for name, func in functions.items():
            if skip[name]:
                continue
            start = time()
            func(data)
            end = time()
            durations[name][idx] = end-start
            print(f'    {name}: {durations[name][idx]}')
            if durations[name][idx] > 100:
                skip[name] = True

    fig, ax = plt.subplots()
    ax.set_title('Speed comparison of implementations')
    ax.set_xscale('log')
    ax.set_xlabel('$n$')
    ax.set_yscale('log')
    ax.set_ylabel('time (s)')
    for name, duration in durations.items():
        ax.plot(N, duration, marker='+', ms=12, label=name)
    ax.legend(loc='lower right')
    ax.grid(which='major', color='black', alpha=.5, linestyle='-')
    ax.grid(which='minor', color='black', alpha=.25, linestyle=':')
    fig.savefig('case-study-1-benchmark.png')  # or .pdf or .svg
```

</details>

![](/img/case-study-1-benchmark.svg)

## Conclusion

And so we've gone from an **R**-inspired string-based implementation past a clean and minimal pure-Python one all the way to a fully optimized and parallelized implementation that can scale over many cores. I've had a lot of fun figuring out all these optimizations, and I hope you enjoyed coming along on this optimization journey with me.

Finally, I would like to thank my colleague for being open to having me tinker on their code.